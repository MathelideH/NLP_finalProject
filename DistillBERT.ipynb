{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N87BWFCwQA-F"
   },
   "source": [
    "# **Spring 2023 NLP Homework 5: Fine-tuning Neural Language Models**\n",
    "\n",
    "In this homework you will finetune a neural langauge model to perform the author classification task from HW4. As a reminder, the classifier has to guess which of the following three authors wrote some given text:\n",
    "\n",
    "- Lewis Carrol\n",
    "- Marion Zimmer Bradley\n",
    "- Edgar Allen Poe\n",
    "\n",
    "You will use the DistilBertForSequenceClassification model that you worked with in class. The code for training the model will be identical to the code you worked with in [this notebook](https://colab.research.google.com/drive/1lFrpDzxGIRQYnuwNKAI5Syr5gdOfbWxK?usp=sharing). Your main tasks in this homework are to: \n",
    "\n",
    "1. Convert the data to a format that is appropriate to pass into the model. \n",
    "2. Convert the predictions of the model in a format that makes it possible to compute accuracy, precision, recall and f-scores. (You should be able to reuse the functions to compute these metrics from HW5)\n",
    "\n",
    "#### **What should I do if I run out of RAM?**\n",
    "The free GPUs that Colab assigns might not always reliable. Sometimes you code will run without issues, and other times you might run into RAM errors. For this reason, try to train your models on as much data as possible, but do not worry if you are not able to train it on all of the data. You can also try to run the models on your personal computers without using GPUs! Just make sure to upload the correct .ipynb with outputs to Gradescope. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7CraTYkG0o3"
   },
   "source": [
    "##**Setup**\n",
    "\n",
    "You will be using the [same set of texts](https://drive.google.com/drive/folders/1WG2YWyq7c4CUgYnO2SsC46_jRWXIYTpV?usp=sharing) as in HW5. Upload the all of the .txt files to your Colab repository and specify the directory location in the code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UlxtbvkcHJon",
    "outputId": "aacebb53-5575-4374-c156-bd0f3784ec3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/yruan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#Store data directory in a variable and only use this variable in your code\n",
    "dat_dir = './' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y_0dmicbHK9A"
   },
   "source": [
    "Install and load necessary models and packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "on-g17_wHR7W",
    "outputId": "d22e4d2f-e765-496d-8b9b-e03f0b876b43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (4.27.4)\n",
      "Requirement already satisfied: datasets in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (2.11.0)\n",
      "Requirement already satisfied: pynvml in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (11.5.0)\n",
      "Requirement already satisfied: accelerate in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (0.18.0)\n",
      "Requirement already satisfied: filelock in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: requests in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: xxhash in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: responses<0.19 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: psutil in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from accelerate) (5.9.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from accelerate) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: sympy in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (1.11.1)\n",
      "Requirement already satisfied: networkx in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (3.1)\n",
      "Requirement already satisfied: jinja2 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from torch>=1.4.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (65.6.3)\n",
      "Requirement already satisfied: wheel in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.4.0->accelerate) (0.38.4)\n",
      "Requirement already satisfied: cmake in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (3.26.1)\n",
      "Requirement already satisfied: lit in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.4.0->accelerate) (16.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from jinja2->torch>=1.4.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages (from sympy->torch>=1.4.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets pynvml accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "W2VGz3pGJhAI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from datasets import load_dataset\n",
    "from multiprocessing import cpu_count\n",
    "import numpy as np\n",
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "from accelerate.utils import find_executable_batch_size\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer, logging\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o2mkW1ibJ7hF",
    "outputId": "5032d82e-22cf-48b0-d266-bf45c3ebb745"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Set \"device\" value depending on whether or not you have access to GPUs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "accelerator = Accelerator()\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\", truncation=True, do_lower_case=True)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7POmKsm4KiW9"
   },
   "source": [
    "##**Data pre-processing**\n",
    "\n",
    "Start by writing a function called load_data that returns three lists, one each for train, test and dev. The lists should be formated in a format that the tokenize_function can use -- i.e. the lists should contain pairs of text and labels. (Look at the tokenize_function for further clues on how this should be organized). \n",
    "\n",
    "Feel free to write additional functions to pre-process the data before passing it into load_data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "tfxmsktEK2lO"
   },
   "outputs": [],
   "source": [
    "def load_data(dat_dir):                                 \n",
    "    fnames = glob.glob(dat_dir + '*.txt')\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "    for fname in fnames:\n",
    "        author = fname.split('_')[0].replace(dat_dir, '')\n",
    "        data = []\n",
    "        with open(fname) as f:\n",
    "            for line in f:\n",
    "                if len(line.strip()) > 0:\n",
    "                    data.append({'label': author, 'sent':line.strip()})\n",
    "        train.extend(data[:int(len(data)*0.8)])\n",
    "        dev.extend(data[int(len(data)*0.8):int(len(data)*0.9)])\n",
    "        test.extend(data[int(len(data)*0.9):])\n",
    "    return train, dev, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qe_pYoooLdYG"
   },
   "source": [
    "You will need to tokenize your data before passing it into your model. You can use the following function for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "RzyfFYBGMWCC"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "  lab_to_num = {\n",
    "      'carrol': 0,\n",
    "      'bradley': 1,\n",
    "      'poe': 2\n",
    "  }\n",
    "  #the tokenizer is cached in memory, so will not re-download for every function call. \n",
    "  tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\",\n",
    "                                                      truncation=True,\n",
    "                                                      do_lower_case=True)\n",
    "  tokenized = tokenizer(example['sent'],\n",
    "                        padding = 'max_length',\n",
    "                        return_tensors='pt') #returns dict\n",
    "  # convert label to a tensor and add it to the tokenized.\n",
    "  lab = lab_to_num[example['label']]\n",
    "  tokenized['labels'] = torch.tensor(int(lab)).to(device)\n",
    "\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOXm_53lNGfk"
   },
   "source": [
    "##**Code setup to train and get predictions from the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "3vCSlCJkNPJx"
   },
   "outputs": [],
   "source": [
    "def train(model, tokenized_data, args):\n",
    "  num_epochs = args['num_epochs']\n",
    "  batch_size = args['batch_size']\n",
    "\n",
    "  # Set up the optimizer\n",
    "  optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "  # Set up a dataloader, which will divide the data into batches\n",
    "  train_dataloader = DataLoader(\n",
    "      tokenized_data, shuffle=True, batch_size=batch_size\n",
    "      )\n",
    "\n",
    "  num_training_steps = num_epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\"linear\",\n",
    "                               optimizer=optimizer,\n",
    "                               num_warmup_steps=0,\n",
    "                               num_training_steps=num_training_steps,\n",
    "                               )\n",
    "  #Start train\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "  for epoch in range(num_epochs):\n",
    "    print(\"Epoch\",epoch)\n",
    "    for i,batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        input_ids = batch['input_ids'].squeeze()\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #forward pass\n",
    "        outputs = model(input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        #compute loss and update weights\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "          \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "vcBCna1JNhdJ"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, tokenized_dataset, tokenizer, n):\n",
    "  \"\"\"\n",
    "  n: number of examples from the dataset you want predictions for\n",
    "  \"\"\"\n",
    "  preds = []\n",
    "  eval_dataset = DataLoader(tokenized_dataset[:n], batch_size=1, shuffle=False)\n",
    "  for i,batch in enumerate(eval_dataset):                \n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    input_ids = batch['input_ids'].squeeze()\n",
    "    attention_mask = batch['attention_mask']\n",
    "    labels = batch['labels']\n",
    "    outputs = model(input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    best = torch.argmax(logits)\n",
    "    pred = best.item()\n",
    "\n",
    "    preds.append({'sent': tokenizer.decode(batch[\"input_ids\"][0][0]),\n",
    "                  'pred': pred,\n",
    "                  'gold': batch[\"labels\"][0].item(),\n",
    "                  'logits': outputs.logits})\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07TGj07uPBe6"
   },
   "source": [
    "##**Defining evaluation metrics**\n",
    "\n",
    "Write functions to compute accuracy, precision, recall and fscore. You should be able to re-use the functions you wrote for HW5. You will want to either modify the functions to take as input predictions in the format outputted by get_predictions or write another function to convert the output of get_predictions into a list of predictions and gold_labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "yVSA-ydcFMjU"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def make_confusion_matrix(predictions):\n",
    "    output_labels = []\n",
    "    gold_labels = []\n",
    "    for item in predictions:\n",
    "        output_labels.append(item['pred'])\n",
    "        gold_labels.append(item['gold'])\n",
    "    return np.array(confusion_matrix(gold_labels, output_labels, labels=list(set(gold_labels))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "miGulqrPP_S_"
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate accuracy\n",
    "def calc_accuracy(predictions, average_type='macro'):\n",
    "  cfm = make_confusion_matrix(predictions)\n",
    "  tp = np.array([cfm[i][i] for i in range(len(cfm))])\n",
    "  gold_size = np.sum(cfm,axis=1)\n",
    "  accuracies = np.divide(tp, gold_size)\n",
    "  \n",
    "  if average_type == 'macro':\n",
    "    return np.mean(accuracies)\n",
    "  else:\n",
    "    return np.sum(tp)/np.sum(gold_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "YjT9dId_QB0d"
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate precision\n",
    "def calc_precision(predictions, average_type='macro'):\n",
    "  cfm = make_confusion_matrix(predictions)\n",
    "  tp = np.array([cfm[i][i] for i in range(len(cfm))])\n",
    "  output_size = np.sum(cfm,axis=0)\n",
    "  precisions = []\n",
    "  for i in range(len(cfm)):\n",
    "    if output_size[i]==0:\n",
    "      precisions.append(0)\n",
    "    else:\n",
    "      precisions.append(tp[i]/ output_size[i])\n",
    "  \n",
    "  if average_type == 'macro':\n",
    "    return np.mean(precisions)\n",
    "  else:\n",
    "    return np.sum(tp)/np.sum(output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "7eIxMF9GQGKA"
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate recall\n",
    "def calc_recall(predictions, average_type='macro'):\n",
    "  cfm = make_confusion_matrix(predictions)\n",
    "  tp = np.array([cfm[i][i] for i in range(len(cfm))])\n",
    "  size = np.array([sum([cfm[i][j] for j in range(len(cfm))]) for i in range(len(cfm))])\n",
    "  recalls = np.divide(tp, size)\n",
    "  \n",
    "  if average_type == 'macro':\n",
    "    return np.mean(recalls)\n",
    "  else:\n",
    "    return np.sum(tp)/np.sum(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "9_2tKJHNQIuI"
   },
   "outputs": [],
   "source": [
    "# Write a function to calculate fscore\n",
    "def calc_fscore(precision, recall, beta):\n",
    "  beta = beta**2\n",
    "  return ((beta + 1)*precision*recall)/(beta*precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8ytMFvYhFPu6"
   },
   "outputs": [],
   "source": [
    "def print_scores(model_type, preds):\n",
    "  print(model_type)\n",
    "  print('-------------------------')\n",
    "  precision = calc_precision(preds, \"macro\")\n",
    "  recall = calc_recall(preds,  \"macro\")\n",
    "  accuracy = calc_accuracy(preds, \"micro\")\n",
    "  f1 = calc_fscore(precision, recall, 1)\n",
    "  f2 = calc_fscore(precision, recall, 2)\n",
    "  print('Precision\\t', round(precision, 3))\n",
    "  print('Recall\\t\\t', round(recall, 3))\n",
    "  print('Accuracy\\t', round(accuracy, 3))\n",
    "  print('F2\\t\\t', round(f2, 3))\n",
    "  print('F1\\t\\t', round(f1,3))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "az-f6aTwS2km"
   },
   "source": [
    "##**Evaluating the pre-trained model prior to fine-tuning**\n",
    "\n",
    "Start by loading in the train, validation and test data, and tokenizing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "AXwQ75vETJo6"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "fnames = glob.glob(dat_dir + '*.txt')\n",
    "fnames = [fname for fname in fnames if 'glove' not in fname]\n",
    "\n",
    "# Write your code here to load train, dev and test data. \n",
    "train_dat, dev_dat, test_dat = load_data(dat_dir)\n",
    "\n",
    "# Shuffle training, dev and test\n",
    "random.shuffle(train_dat)\n",
    "random.shuffle(dev_dat)\n",
    "random.shuffle(test_dat)\n",
    "\n",
    "# Create tokenized train, dev and test. \n",
    "## You might want to look at only a small subset of train, dev and test to avoid RAM issues. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVUuUt_ZFX6A",
    "outputId": "2505654b-37d7-47e5-ef21-c26993560f6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in Train\n",
      "bradley 7774\n",
      "poe 7903\n",
      "carrol 8203\n",
      "Total:  23880\n",
      "\n",
      "Number of sentences in Dev\n",
      "bradley 972\n",
      "poe 988\n",
      "carrol 1025\n",
      "Total:  2985\n",
      "\n",
      "Number of sentences in Test\n",
      "bradley 974\n",
      "poe 990\n",
      "carrol 1028\n",
      "Total:  2992\n"
     ]
    }
   ],
   "source": [
    "# Sanity check on the train, dev, and test sets\n",
    "print('Number of sentences in Train')\n",
    "count = {}\n",
    "count['bradley'] = 0\n",
    "count['poe'] = 0\n",
    "count['carrol'] = 0\n",
    "for d in train_dat:\n",
    "    count[d['label']] += 1\n",
    "for key,val in count.items():\n",
    "  print(key, val)\n",
    "print('Total: ', len(train_dat))\n",
    "\n",
    "print()\n",
    "print('Number of sentences in Dev')\n",
    "count = {}\n",
    "count['bradley'] = 0\n",
    "count['poe'] = 0\n",
    "count['carrol'] = 0\n",
    "for d in dev_dat:\n",
    "    count[d['label']] += 1\n",
    "for key,val in count.items():\n",
    "  print(key, val)\n",
    "print('Total: ', len(dev_dat))\n",
    "\n",
    "print()\n",
    "print('Number of sentences in Test')\n",
    "count = {}\n",
    "count['bradley'] = 0\n",
    "count['poe'] = 0\n",
    "count['carrol'] = 0\n",
    "for d in test_dat:\n",
    "    count[d['label']] += 1\n",
    "for key,val in count.items():\n",
    "  print(key, val)\n",
    "print('Total: ', len(test_dat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaeL578pTgWi"
   },
   "source": [
    "Load the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCsweiLcTjRL",
    "outputId": "b329faee-9a2e-493d-9bb4-1a5bd103ab85"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-cased\",\n",
    "                                                            num_labels=3).to(device)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-cased\",\n",
    "                                                      truncation=True,\n",
    "                                                      do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJ4iWa0cTtZ-"
   },
   "source": [
    "Evaluate the model on the test set prior to fine-tuning. If you run into RAM issues, evaluate it on a smaller set using the n parameter of get_predictions(). Make sure to print precision, accuracy, recall and f1 in an easy to read format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "OnvBeIEPT2l8"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "tokenized_train = [tokenize_function(e) for e in train_dat[:500]]\n",
    "tokenized_test = [tokenize_function(t) for t in test_dat[:500]]\n",
    "tokenized_dev = [tokenize_function(d) for d in dev_dat[:500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "tueeD4FZGQOD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model prior to fine-tuning yields these scores\n",
      "-------------------------\n",
      "Precision\t 0.102\n",
      "Recall\t\t 0.333\n",
      "Accuracy\t 0.307\n",
      "F2\t\t 0.23\n",
      "F1\t\t 0.156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_before = get_predictions(model, tokenized_test, tokenizer, 150)\n",
    "print_scores(\"model prior to fine-tuning yields these scores\", preds_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GutlPhwBT-8V"
   },
   "source": [
    "##**Fine-tuning the model**\n",
    "\n",
    "Fine tune the model to the training dataset (or subsets of the dataset) and save it using `torch.save()`. Set the number of epochs to three, and the batch_size to 5. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5uL_rbwmW44D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/miniconda3/envs/jupiterhub/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 1/300 [00:02<11:53,  2.39s/it]\n",
      "  1%|          | 2/300 [00:04<11:17,  2.27s/it]\u001b[A\n",
      "  1%|          | 3/300 [00:06<10:05,  2.04s/it]\u001b[A\n",
      "  1%|▏         | 4/300 [00:08<10:44,  2.18s/it]\u001b[A\n",
      "  2%|▏         | 5/300 [00:10<09:58,  2.03s/it]\u001b[A\n",
      "  2%|▏         | 6/300 [00:11<09:03,  1.85s/it]\u001b[A\n",
      "  2%|▏         | 7/300 [00:13<08:15,  1.69s/it]\u001b[A\n",
      "  3%|▎         | 8/300 [00:15<08:26,  1.74s/it]\u001b[A\n",
      "  3%|▎         | 9/300 [00:16<08:08,  1.68s/it]\u001b[A\n",
      "  3%|▎         | 10/300 [00:18<07:48,  1.62s/it][A\n",
      "  4%|▎         | 11/300 [00:19<07:50,  1.63s/it]\u001b[A\n",
      "  4%|▍         | 12/300 [00:21<08:00,  1.67s/it]\u001b[A\n",
      "  4%|▍         | 13/300 [00:23<07:57,  1.66s/it]\u001b[A\n",
      "  5%|▍         | 14/300 [00:24<07:44,  1.63s/it]\u001b[A\n",
      "  5%|▌         | 15/300 [00:26<07:24,  1.56s/it]\u001b[A\n",
      "  5%|▌         | 16/300 [00:27<07:17,  1.54s/it]\u001b[A\n",
      "  6%|▌         | 17/300 [00:29<07:08,  1.51s/it]\u001b[A\n",
      "  6%|▌         | 18/300 [00:30<07:07,  1.52s/it]\u001b[A\n",
      "  6%|▋         | 19/300 [00:32<07:59,  1.71s/it]\u001b[A\n",
      "  7%|▋         | 20/300 [00:34<08:20,  1.79s/it]\u001b[A\n",
      "  7%|▋         | 21/300 [00:36<08:17,  1.78s/it]\u001b[A\n",
      "  7%|▋         | 22/300 [00:38<07:53,  1.70s/it]\u001b[A\n",
      "  8%|▊         | 23/300 [00:39<07:24,  1.61s/it]\u001b[A\n",
      "  8%|▊         | 24/300 [00:41<07:15,  1.58s/it]\u001b[A\n",
      "  8%|▊         | 25/300 [00:42<07:07,  1.55s/it]\u001b[A\n",
      "  9%|▊         | 26/300 [00:43<06:58,  1.53s/it]\u001b[A\n",
      "  9%|▉         | 27/300 [00:45<07:09,  1.57s/it]\u001b[A\n",
      "  9%|▉         | 28/300 [00:47<07:06,  1.57s/it]\u001b[A\n",
      " 10%|▉         | 29/300 [00:49<07:25,  1.64s/it]\u001b[A\n",
      " 10%|█         | 30/300 [00:50<07:27,  1.66s/it]\u001b[A\n",
      " 10%|█         | 31/300 [00:52<07:19,  1.63s/it]\u001b[A\n",
      " 11%|█         | 32/300 [00:53<06:59,  1.56s/it]\u001b[A\n",
      " 11%|█         | 33/300 [00:55<06:48,  1.53s/it]\u001b[A\n",
      " 11%|█▏        | 34/300 [00:56<06:38,  1.50s/it]\u001b[A\n",
      " 12%|█▏        | 35/300 [00:58<06:37,  1.50s/it]\u001b[A\n",
      " 12%|█▏        | 36/300 [00:59<06:28,  1.47s/it]\u001b[A\n",
      " 12%|█▏        | 37/300 [01:01<07:03,  1.61s/it]\u001b[A\n",
      " 13%|█▎        | 38/300 [01:03<07:27,  1.71s/it]\u001b[A\n",
      " 13%|█▎        | 39/300 [01:04<07:11,  1.65s/it]\u001b[A\n",
      " 13%|█▎        | 40/300 [01:06<06:49,  1.58s/it]\u001b[A\n",
      " 14%|█▎        | 41/300 [01:07<06:22,  1.47s/it]\u001b[A\n",
      " 14%|█▍        | 42/300 [01:09<06:30,  1.51s/it]\u001b[A\n",
      " 14%|█▍        | 43/300 [01:10<06:30,  1.52s/it]\u001b[A\n",
      " 15%|█▍        | 44/300 [01:12<06:17,  1.48s/it]\u001b[A\n",
      " 15%|█▌        | 45/300 [01:13<06:06,  1.44s/it]\u001b[A\n",
      " 15%|█▌        | 46/300 [01:15<06:26,  1.52s/it]\u001b[A\n",
      " 16%|█▌        | 47/300 [01:17<07:07,  1.69s/it]\u001b[A\n",
      " 16%|█▌        | 48/300 [01:18<06:37,  1.58s/it]\u001b[A\n",
      " 16%|█▋        | 49/300 [01:19<06:25,  1.53s/it]\u001b[A\n",
      " 17%|█▋        | 50/300 [01:21<06:19,  1.52s/it]\u001b[A\n",
      " 17%|█▋        | 51/300 [01:23<06:27,  1.56s/it]\u001b[A\n",
      " 17%|█▋        | 52/300 [01:24<06:14,  1.51s/it]\u001b[A\n",
      " 18%|█▊        | 53/300 [01:25<06:07,  1.49s/it]\u001b[A\n",
      " 18%|█▊        | 54/300 [01:27<06:06,  1.49s/it]\u001b[A\n",
      " 18%|█▊        | 55/300 [01:29<06:16,  1.53s/it]\u001b[A\n",
      " 19%|█▊        | 56/300 [01:30<06:27,  1.59s/it]\u001b[A\n",
      " 19%|█▉        | 57/300 [01:32<06:33,  1.62s/it]\u001b[A\n",
      " 19%|█▉        | 58/300 [01:33<06:15,  1.55s/it]\u001b[A\n",
      " 20%|█▉        | 59/300 [01:35<06:06,  1.52s/it]\u001b[A\n",
      " 20%|██        | 60/300 [01:36<06:14,  1.56s/it]\u001b[A\n",
      " 20%|██        | 61/300 [01:38<06:08,  1.54s/it]\u001b[A\n",
      " 21%|██        | 62/300 [01:40<06:12,  1.57s/it]\u001b[A\n",
      " 21%|██        | 63/300 [01:41<05:56,  1.51s/it]\u001b[A\n",
      " 21%|██▏       | 64/300 [01:43<06:09,  1.57s/it]\u001b[A\n",
      " 22%|██▏       | 65/300 [01:44<06:08,  1.57s/it]\u001b[A\n",
      " 22%|██▏       | 66/300 [01:46<05:53,  1.51s/it]\u001b[A\n",
      " 22%|██▏       | 67/300 [01:47<05:51,  1.51s/it]\u001b[A\n",
      " 23%|██▎       | 68/300 [01:49<05:46,  1.49s/it]\u001b[A\n",
      " 23%|██▎       | 69/300 [01:50<06:02,  1.57s/it]\u001b[A\n",
      " 23%|██▎       | 70/300 [01:52<05:56,  1.55s/it]\u001b[A\n",
      " 24%|██▎       | 71/300 [01:53<05:47,  1.52s/it]\u001b[A\n",
      " 24%|██▍       | 72/300 [01:55<05:38,  1.48s/it]\u001b[A\n",
      " 24%|██▍       | 73/300 [01:56<05:39,  1.50s/it]\u001b[A\n",
      " 25%|██▍       | 74/300 [01:58<05:54,  1.57s/it]\u001b[A\n",
      " 25%|██▌       | 75/300 [01:59<05:44,  1.53s/it]\u001b[A\n",
      " 25%|██▌       | 76/300 [02:01<05:36,  1.50s/it]\u001b[A\n",
      " 26%|██▌       | 77/300 [02:02<05:13,  1.40s/it]\u001b[A\n",
      " 26%|██▌       | 78/300 [02:03<05:09,  1.39s/it]\u001b[A\n",
      " 26%|██▋       | 79/300 [02:05<05:18,  1.44s/it]\u001b[A\n",
      " 27%|██▋       | 80/300 [02:06<05:25,  1.48s/it]\u001b[A\n",
      " 27%|██▋       | 81/300 [02:08<05:13,  1.43s/it]\u001b[A\n",
      " 27%|██▋       | 82/300 [02:09<05:21,  1.47s/it]\u001b[A\n",
      " 28%|██▊       | 83/300 [02:11<05:38,  1.56s/it]\u001b[A\n",
      " 28%|██▊       | 84/300 [02:13<05:42,  1.59s/it]\u001b[A\n",
      " 28%|██▊       | 85/300 [02:14<05:30,  1.54s/it]\u001b[A\n",
      " 29%|██▊       | 86/300 [02:16<05:20,  1.50s/it]\u001b[A\n",
      " 29%|██▉       | 87/300 [02:17<05:27,  1.54s/it]\u001b[A\n",
      " 29%|██▉       | 88/300 [02:19<05:25,  1.53s/it]\u001b[A\n",
      " 30%|██▉       | 89/300 [02:20<05:25,  1.54s/it]\u001b[A\n",
      " 30%|███       | 90/300 [02:22<05:08,  1.47s/it]\u001b[A\n",
      " 30%|███       | 91/300 [02:23<05:21,  1.54s/it]\u001b[A\n",
      " 31%|███       | 92/300 [02:25<05:36,  1.62s/it]\u001b[A\n",
      " 31%|███       | 93/300 [02:27<05:57,  1.73s/it]\u001b[A\n",
      " 31%|███▏      | 94/300 [02:29<06:10,  1.80s/it]\u001b[A\n",
      " 32%|███▏      | 95/300 [02:31<05:51,  1.72s/it]\u001b[A\n",
      " 32%|███▏      | 96/300 [02:32<05:36,  1.65s/it]\u001b[A\n",
      " 32%|███▏      | 97/300 [02:33<05:20,  1.58s/it]\u001b[A\n",
      " 33%|███▎      | 98/300 [02:35<05:08,  1.53s/it]\u001b[A\n",
      " 33%|███▎      | 99/300 [02:36<05:06,  1.53s/it]\u001b[A\n",
      " 33%|███▎      | 100/300 [02:38<05:27,  1.64s/it][A\n",
      "100%|██████████| 100/100 [02:38<00:00,  1.59s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 34%|███▎      | 101/300 [02:40<05:17,  1.60s/it]\n",
      " 34%|███▍      | 102/300 [02:41<05:07,  1.55s/it]A\n",
      " 34%|███▍      | 103/300 [02:42<04:48,  1.46s/it]A\n",
      " 35%|███▍      | 104/300 [02:44<04:42,  1.44s/it]A\n",
      " 35%|███▌      | 105/300 [02:45<04:51,  1.49s/it]A\n",
      " 35%|███▌      | 106/300 [02:47<04:48,  1.49s/it]A\n",
      " 36%|███▌      | 107/300 [02:48<04:40,  1.46s/it]A\n",
      " 36%|███▌      | 108/300 [02:50<04:37,  1.44s/it]A\n",
      " 36%|███▋      | 109/300 [02:51<04:41,  1.48s/it]A\n",
      " 37%|███▋      | 110/300 [02:53<05:04,  1.60s/it]A\n",
      " 37%|███▋      | 111/300 [02:55<05:02,  1.60s/it][A\n",
      " 37%|███▋      | 112/300 [02:56<04:46,  1.52s/it][A\n",
      " 38%|███▊      | 113/300 [02:58<04:44,  1.52s/it][A\n",
      " 38%|███▊      | 114/300 [02:59<04:42,  1.52s/it][A\n",
      " 38%|███▊      | 115/300 [03:01<04:40,  1.52s/it][A\n",
      " 39%|███▊      | 116/300 [03:02<04:31,  1.48s/it][A\n",
      " 39%|███▉      | 117/300 [03:03<04:24,  1.45s/it][A\n",
      " 39%|███▉      | 118/300 [03:05<04:41,  1.55s/it][A\n",
      " 40%|███▉      | 119/300 [03:07<04:40,  1.55s/it][A\n",
      " 40%|████      | 120/300 [03:08<04:31,  1.51s/it][A\n",
      " 40%|████      | 121/300 [03:10<04:20,  1.46s/it][A\n",
      " 41%|████      | 122/300 [03:11<04:24,  1.49s/it][A\n",
      " 41%|████      | 123/300 [03:13<04:38,  1.57s/it][A\n",
      " 41%|████▏     | 124/300 [03:14<04:22,  1.49s/it][A\n",
      " 42%|████▏     | 125/300 [03:16<04:18,  1.48s/it][A\n",
      " 42%|████▏     | 126/300 [03:17<04:09,  1.44s/it][A\n",
      " 42%|████▏     | 127/300 [03:19<04:14,  1.47s/it][A\n",
      " 43%|████▎     | 128/300 [03:20<04:27,  1.56s/it][A\n",
      " 43%|████▎     | 129/300 [03:22<04:22,  1.53s/it][A\n",
      " 43%|████▎     | 130/300 [03:23<04:13,  1.49s/it][A\n",
      " 44%|████▎     | 131/300 [03:24<04:00,  1.42s/it][A\n",
      " 44%|████▍     | 132/300 [03:26<03:53,  1.39s/it][A\n",
      " 44%|████▍     | 133/300 [03:27<04:00,  1.44s/it][A\n",
      " 45%|████▍     | 134/300 [03:29<03:56,  1.43s/it][A\n",
      " 45%|████▌     | 135/300 [03:30<03:55,  1.43s/it][A\n",
      " 45%|████▌     | 136/300 [03:31<03:48,  1.39s/it][A\n",
      " 46%|████▌     | 137/300 [03:33<03:56,  1.45s/it][A\n",
      " 46%|████▌     | 138/300 [03:35<04:15,  1.58s/it][A\n",
      " 46%|████▋     | 139/300 [03:37<04:30,  1.68s/it][A\n",
      " 47%|████▋     | 140/300 [03:38<04:17,  1.61s/it][A\n",
      " 47%|████▋     | 141/300 [03:39<03:59,  1.50s/it][A\n",
      " 47%|████▋     | 142/300 [03:41<04:12,  1.60s/it][A\n",
      " 48%|████▊     | 143/300 [03:43<04:01,  1.54s/it][A\n",
      " 48%|████▊     | 144/300 [03:44<03:52,  1.49s/it][A\n",
      " 48%|████▊     | 145/300 [03:46<03:48,  1.47s/it][A\n",
      " 49%|████▊     | 146/300 [03:47<03:52,  1.51s/it][A\n",
      " 49%|████▉     | 147/300 [03:49<03:58,  1.56s/it][A\n",
      " 49%|████▉     | 148/300 [03:50<03:54,  1.54s/it][A\n",
      " 50%|████▉     | 149/300 [03:52<03:49,  1.52s/it][A\n",
      " 50%|█████     | 150/300 [03:53<03:42,  1.48s/it][A\n",
      " 50%|█████     | 151/300 [03:55<03:41,  1.49s/it][A\n",
      " 51%|█████     | 152/300 [03:56<03:39,  1.48s/it][A\n",
      " 51%|█████     | 153/300 [03:57<03:30,  1.43s/it][A\n",
      " 51%|█████▏    | 154/300 [03:59<03:26,  1.41s/it][A\n",
      " 52%|█████▏    | 155/300 [04:01<03:41,  1.53s/it][A\n",
      " 52%|█████▏    | 156/300 [04:02<03:52,  1.62s/it][A\n",
      " 52%|█████▏    | 157/300 [04:04<03:39,  1.53s/it][A\n",
      " 53%|█████▎    | 158/300 [04:05<03:31,  1.49s/it][A\n",
      " 53%|█████▎    | 159/300 [04:06<03:22,  1.43s/it][A\n",
      " 53%|█████▎    | 160/300 [04:08<03:19,  1.43s/it][A\n",
      " 54%|█████▎    | 161/300 [04:09<03:17,  1.42s/it][A\n",
      " 54%|█████▍    | 162/300 [04:11<03:18,  1.44s/it][A\n",
      " 54%|█████▍    | 163/300 [04:12<03:23,  1.49s/it][A\n",
      " 55%|█████▍    | 164/300 [04:14<03:22,  1.49s/it][A\n",
      " 55%|█████▌    | 165/300 [04:16<03:32,  1.57s/it][A\n",
      " 55%|█████▌    | 166/300 [04:17<03:31,  1.58s/it][A\n",
      " 56%|█████▌    | 167/300 [04:19<03:20,  1.51s/it][A\n",
      " 56%|█████▌    | 168/300 [04:20<03:07,  1.42s/it][A\n",
      " 56%|█████▋    | 169/300 [04:21<03:05,  1.42s/it][A\n",
      " 57%|█████▋    | 170/300 [04:23<03:05,  1.43s/it][A\n",
      " 57%|█████▋    | 171/300 [04:24<03:06,  1.44s/it][A\n",
      " 57%|█████▋    | 172/300 [04:26<03:06,  1.46s/it][A\n",
      " 58%|█████▊    | 173/300 [04:27<03:06,  1.47s/it][A\n",
      " 58%|█████▊    | 174/300 [04:29<03:14,  1.54s/it][A\n",
      " 58%|█████▊    | 175/300 [04:30<03:10,  1.52s/it][A\n",
      " 59%|█████▊    | 176/300 [04:32<03:02,  1.47s/it][A\n",
      " 59%|█████▉    | 177/300 [04:33<03:11,  1.56s/it][A\n",
      " 59%|█████▉    | 178/300 [04:35<03:06,  1.53s/it][A\n",
      " 60%|█████▉    | 179/300 [04:36<02:56,  1.46s/it][A\n",
      " 60%|██████    | 180/300 [04:38<03:00,  1.50s/it][A\n",
      " 60%|██████    | 181/300 [04:39<02:52,  1.45s/it][A\n",
      " 61%|██████    | 182/300 [04:40<02:49,  1.43s/it][A\n",
      " 61%|██████    | 183/300 [04:42<03:07,  1.60s/it][A\n",
      " 61%|██████▏   | 184/300 [04:44<03:08,  1.62s/it][A\n",
      " 62%|██████▏   | 185/300 [04:46<03:02,  1.59s/it][A\n",
      " 62%|██████▏   | 186/300 [04:47<02:55,  1.54s/it][A\n",
      " 62%|██████▏   | 187/300 [04:49<02:55,  1.55s/it][A\n",
      " 63%|██████▎   | 188/300 [04:50<02:51,  1.53s/it][A\n",
      " 63%|██████▎   | 189/300 [04:52<02:46,  1.50s/it][A\n",
      " 63%|██████▎   | 190/300 [04:53<02:33,  1.39s/it][A\n",
      " 64%|██████▎   | 191/300 [04:54<02:29,  1.37s/it][A\n",
      " 64%|██████▍   | 192/300 [04:56<02:43,  1.52s/it][A\n",
      " 64%|██████▍   | 193/300 [04:58<02:50,  1.59s/it][A\n",
      " 65%|██████▍   | 194/300 [04:59<02:42,  1.53s/it][A\n",
      " 65%|██████▌   | 195/300 [05:00<02:35,  1.48s/it][A\n",
      " 65%|██████▌   | 196/300 [05:02<02:40,  1.55s/it][A\n",
      " 66%|██████▌   | 197/300 [05:04<02:35,  1.51s/it][A\n",
      " 66%|██████▌   | 198/300 [05:05<02:29,  1.46s/it][A\n",
      " 66%|██████▋   | 199/300 [05:06<02:25,  1.44s/it][A\n",
      " 67%|██████▋   | 200/300 [05:08<02:22,  1.43s/it][A\n",
      "100%|██████████| 100/100 [02:29<00:00,  1.49s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 67%|██████▋   | 201/300 [05:09<02:27,  1.49s/it]\n",
      " 67%|██████▋   | 202/300 [05:11<02:33,  1.56s/it]A\n",
      " 68%|██████▊   | 203/300 [05:12<02:26,  1.51s/it]A\n",
      " 68%|██████▊   | 204/300 [05:14<02:17,  1.44s/it]A\n",
      " 68%|██████▊   | 205/300 [05:15<02:13,  1.41s/it]A\n",
      " 69%|██████▊   | 206/300 [05:17<02:17,  1.46s/it]A\n",
      " 69%|██████▉   | 207/300 [05:18<02:11,  1.41s/it]A\n",
      " 69%|██████▉   | 208/300 [05:19<02:10,  1.42s/it]A\n",
      " 70%|██████▉   | 209/300 [05:21<02:05,  1.38s/it]A\n",
      " 70%|███████   | 210/300 [05:22<02:03,  1.37s/it]A\n",
      " 70%|███████   | 211/300 [05:24<02:09,  1.45s/it][A\n",
      " 71%|███████   | 212/300 [05:25<02:06,  1.44s/it][A\n",
      " 71%|███████   | 213/300 [05:26<02:01,  1.40s/it][A\n",
      " 71%|███████▏  | 214/300 [05:28<02:00,  1.40s/it][A\n",
      " 72%|███████▏  | 215/300 [05:29<01:54,  1.34s/it][A\n",
      " 72%|███████▏  | 216/300 [05:31<01:57,  1.40s/it][A\n",
      " 72%|███████▏  | 217/300 [05:32<01:56,  1.40s/it][A\n",
      " 73%|███████▎  | 218/300 [05:33<01:53,  1.38s/it][A\n",
      " 73%|███████▎  | 219/300 [05:35<01:50,  1.37s/it][A\n",
      " 73%|███████▎  | 220/300 [05:36<01:52,  1.41s/it][A\n",
      " 74%|███████▎  | 221/300 [05:38<02:01,  1.54s/it][A\n",
      " 74%|███████▍  | 222/300 [05:39<01:56,  1.49s/it][A\n",
      " 74%|███████▍  | 223/300 [05:41<01:55,  1.50s/it][A\n",
      " 75%|███████▍  | 224/300 [05:42<01:51,  1.47s/it][A\n",
      " 75%|███████▌  | 225/300 [05:44<01:52,  1.50s/it][A\n",
      " 75%|███████▌  | 226/300 [05:45<01:52,  1.52s/it][A\n",
      " 76%|███████▌  | 227/300 [05:47<01:52,  1.54s/it][A\n",
      " 76%|███████▌  | 228/300 [05:48<01:46,  1.47s/it][A\n",
      " 76%|███████▋  | 229/300 [05:50<01:41,  1.43s/it][A\n",
      " 77%|███████▋  | 230/300 [05:51<01:48,  1.55s/it][A\n",
      " 77%|███████▋  | 231/300 [05:53<01:48,  1.58s/it][A\n",
      " 77%|███████▋  | 232/300 [05:55<01:44,  1.54s/it][A\n",
      " 78%|███████▊  | 233/300 [05:56<01:43,  1.54s/it][A\n",
      " 78%|███████▊  | 234/300 [05:58<01:39,  1.51s/it][A\n",
      " 78%|███████▊  | 235/300 [05:59<01:33,  1.44s/it][A\n",
      " 79%|███████▊  | 236/300 [06:00<01:34,  1.48s/it][A\n",
      " 79%|███████▉  | 237/300 [06:02<01:31,  1.45s/it][A\n",
      " 79%|███████▉  | 238/300 [06:03<01:29,  1.45s/it][A\n",
      " 80%|███████▉  | 239/300 [06:05<01:29,  1.48s/it][A\n",
      " 80%|████████  | 240/300 [06:07<01:38,  1.64s/it][A\n",
      " 80%|████████  | 241/300 [06:08<01:32,  1.58s/it][A\n",
      " 81%|████████  | 242/300 [06:10<01:27,  1.51s/it][A\n",
      " 81%|████████  | 243/300 [06:11<01:23,  1.46s/it][A\n",
      " 81%|████████▏ | 244/300 [06:12<01:21,  1.45s/it][A\n",
      " 82%|████████▏ | 245/300 [06:14<01:17,  1.40s/it][A\n",
      " 82%|████████▏ | 246/300 [06:15<01:10,  1.31s/it][A\n",
      " 82%|████████▏ | 247/300 [06:16<01:03,  1.20s/it][A\n",
      " 83%|████████▎ | 248/300 [06:17<01:04,  1.24s/it][A\n",
      " 83%|████████▎ | 249/300 [06:18<01:06,  1.30s/it][A\n",
      " 83%|████████▎ | 250/300 [06:20<01:10,  1.40s/it][A\n",
      " 84%|████████▎ | 251/300 [06:21<01:07,  1.37s/it][A\n",
      " 84%|████████▍ | 252/300 [06:23<01:04,  1.35s/it][A\n",
      " 84%|████████▍ | 253/300 [06:24<01:01,  1.30s/it][A\n",
      " 85%|████████▍ | 254/300 [06:25<01:00,  1.32s/it][A\n",
      " 85%|████████▌ | 255/300 [06:27<01:02,  1.38s/it][A\n",
      " 85%|████████▌ | 256/300 [06:28<01:01,  1.40s/it][A\n",
      " 86%|████████▌ | 257/300 [06:30<01:01,  1.42s/it][A\n",
      " 86%|████████▌ | 258/300 [06:31<00:57,  1.38s/it][A\n",
      " 86%|████████▋ | 259/300 [06:33<01:01,  1.49s/it][A\n",
      " 87%|████████▋ | 260/300 [06:34<01:00,  1.50s/it][A\n",
      " 87%|████████▋ | 261/300 [06:36<00:58,  1.49s/it][A\n",
      " 87%|████████▋ | 262/300 [06:37<00:54,  1.44s/it][A\n",
      " 88%|████████▊ | 263/300 [06:38<00:51,  1.39s/it][A\n",
      " 88%|████████▊ | 264/300 [06:40<00:53,  1.48s/it][A\n",
      " 88%|████████▊ | 265/300 [06:41<00:51,  1.47s/it][A\n",
      " 89%|████████▊ | 266/300 [06:43<00:49,  1.47s/it][A\n",
      " 89%|████████▉ | 267/300 [06:44<00:48,  1.48s/it][A\n",
      " 89%|████████▉ | 268/300 [06:46<00:47,  1.48s/it][A\n",
      " 90%|████████▉ | 269/300 [06:48<00:49,  1.59s/it][A\n",
      " 90%|█████████ | 270/300 [06:49<00:48,  1.62s/it][A\n",
      " 90%|█████████ | 271/300 [06:51<00:44,  1.54s/it][A\n",
      " 91%|█████████ | 272/300 [06:52<00:39,  1.42s/it][A\n",
      " 91%|█████████ | 273/300 [06:53<00:38,  1.42s/it][A\n",
      " 91%|█████████▏| 274/300 [06:55<00:36,  1.39s/it][A\n",
      " 92%|█████████▏| 275/300 [06:56<00:34,  1.40s/it][A\n",
      " 92%|█████████▏| 276/300 [06:58<00:33,  1.42s/it][A\n",
      " 92%|█████████▏| 277/300 [06:59<00:32,  1.42s/it][A\n",
      " 93%|█████████▎| 278/300 [07:00<00:31,  1.44s/it][A\n",
      " 93%|█████████▎| 279/300 [07:02<00:33,  1.61s/it][A\n",
      " 93%|█████████▎| 280/300 [07:04<00:32,  1.62s/it][A\n",
      " 94%|█████████▎| 281/300 [07:06<00:29,  1.56s/it][A\n",
      " 94%|█████████▍| 282/300 [07:07<00:26,  1.48s/it][A\n",
      " 94%|█████████▍| 283/300 [07:08<00:25,  1.53s/it][A\n",
      " 95%|█████████▍| 284/300 [07:10<00:23,  1.47s/it][A\n",
      " 95%|█████████▌| 285/300 [07:11<00:21,  1.46s/it][A\n",
      " 95%|█████████▌| 286/300 [07:13<00:19,  1.41s/it][A\n",
      " 96%|█████████▌| 287/300 [07:14<00:19,  1.48s/it][A\n",
      " 96%|█████████▌| 288/300 [07:16<00:18,  1.57s/it][A\n",
      " 96%|█████████▋| 289/300 [07:17<00:16,  1.53s/it][A\n",
      " 97%|█████████▋| 290/300 [07:19<00:14,  1.50s/it][A\n",
      " 97%|█████████▋| 291/300 [07:20<00:13,  1.51s/it][A\n",
      " 97%|█████████▋| 292/300 [07:22<00:11,  1.46s/it][A\n",
      " 98%|█████████▊| 293/300 [07:23<00:10,  1.45s/it][A\n",
      " 98%|█████████▊| 294/300 [07:25<00:08,  1.46s/it][A\n",
      " 98%|█████████▊| 295/300 [07:26<00:07,  1.47s/it][A\n",
      " 99%|█████████▊| 296/300 [07:27<00:05,  1.45s/it][A\n",
      " 99%|█████████▉| 297/300 [07:29<00:04,  1.44s/it][A\n",
      " 99%|█████████▉| 298/300 [07:31<00:03,  1.53s/it][A\n",
      "100%|█████████▉| 299/300 [07:32<00:01,  1.51s/it][A\n",
      "100%|██████████| 300/300 [07:34<00:00,  1.49s/it][A\n",
      "100%|██████████| 100/100 [02:25<00:00,  1.46s/it]\u001b[A\n",
      "100%|██████████| 300/300 [07:34<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'num_epochs': 3,\n",
    "    'batch_size': 5\n",
    "}\n",
    "\n",
    "## Write your code here\n",
    "train(model,tokenized_train, args)\n",
    "torch.save(model, 'model.pt')\n",
    "trained_model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz4rJZ9FXbdp"
   },
   "source": [
    "##**Evaluating the model**\n",
    "\n",
    "Evaluate the saved model on the test set. Make sure to display the evaluation metrics in an easy-to-view format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "GwllbAR9Xkx8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model after fine-tuning yields these scores\n",
      "-------------------------\n",
      "Precision\t 0.403\n",
      "Recall\t\t 0.413\n",
      "Accuracy\t 0.433\n",
      "F2\t\t 0.411\n",
      "F1\t\t 0.408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_after = get_predictions(trained_model, tokenized_test, tokenizer, 150)\n",
    "print_scores(\"model after fine-tuning yields these scores\", preds_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ3XvYJ1XrEB"
   },
   "source": [
    "##**Discussion/ Reflection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1-IhGiHXy7J"
   },
   "source": [
    "####**Comparison to your Bayesian Classifiers**\n",
    "\n",
    "How does the model performance compare to your Bayesian Classifiers? What do you think might contribute to the differences between these two classes of models?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jdKH3m8YXW8"
   },
   "source": [
    "[Write your answer here]\n",
    "\n",
    "The highest accuracy and precision yielded by our best Bayesian Classifier were no higher than 0.42 and 0.37 respectively, while the DistilBERT model yields an accuracy of 0.63 and a precision of 0.812. Based on these metrics, this model is performing much better than our Bayesian classifiers, even though our DistilBERT model were only validated on a small dev_test and tested on a small test_set (due to RAM issues). The DistilBERT model also improves significantly more than the Naive Bayes: hyperparameter tuning only helps NB to improve accuracy by ~10%, while DistilBERT increases accuracy by ～15% after being trained.\n",
    "\n",
    "There are several key advantages of a Transformer (DistilBERT) over Naive-Bayes for classification task:\n",
    "- Benefits of multiple attention heads + encoder-decoder architecture + stacked encoders: This model excels at representation learning - while NB does not generate meaningful representation of inputs, as it only deals with fixed probabilities of inputs computed from n-gram frequencies. The current model is able to learn multiple dependencies through multiple layers with attention masks specifying which aspects of the embeddings are important to keep in mind, thus being able to generalize/predict for longer sequences. Thus, DistiBERT can improve much more at predicting by training and fine-tuning, while NB does not increase its classifying accuracy by as much via hyper-parameter tuning.\n",
    "- Benefit of being bidirectional and implementing [MASK] as a Transformer: Furthermore, one limitations of our bayesian classfiers was that they are limited by a fixed context-window of n-grams (unigrams and bigrams here) when making predictions, while the DistiBERT model are bidirectional, thus can perform the classification based on a much broader scope of data and attend to important contexts. Thus, even when evaluated on a small set, it has much higher accuracy than an n-gram model.\n",
    "- Benefits of positional embeddings: NB assumes a bag-of-word assumption, and does not take into account the words' positions, hence ignoring the temporal nature of language. DistiBERT, through bidirectional representation learning and positional encoding + self-attention, captures the relevance of a tokens in a current context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d79I5pDzW6QG"
   },
   "source": [
    "####**Tuning hyperparameters**\n",
    "What are some of the hyperparameters you could tweak? What process would you use to pick the optimal hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwE5u36rYb1Z"
   },
   "source": [
    "[Write your answer here]\n",
    "We can look into the TrainingArguments of the train() function - these are the hyperparameters that can be tweaked. Twp of them are number of epochs and the batch size. The number of epochs are the number of iterations the current model will perform as it learns over the dataset, while the batch size refers to the number of sentences the model will take to learn during each iteration. In addition, we can also change attributes to customize the training, such as: evaluation_strategy, learning_rate, weight_decay,etc.\n",
    "\n",
    "The first way to find the optimal hyperparameter is that we can pick a range batch size and the number of epochs for the model to learn. For example, 2-5 for the num_epoch and 3-50 taking consideration of both the time of training and the performance of the model. Using these two random number, we will evaluate on the validation (dev) set and identify a pattern between these two hyperparameters and accuracy, precision, recall, and f1-score that can help guide us to the optimal set of batchsize and num_epochs. We can also implement grid search to find the optimal combinations of hyperparameters, which yields the maximum accuracy/F-1 scores.\n",
    "\n",
    "Another way to find the optimal hyperparameters is to to use a loss function for the result of the learning. We can choose a series of epoch numbers to be tested upon the dev data set and compare the total loss for each num_epoch. We can either plot out the loss in a function and search for a local(ideally global) minimum, or manually set an acceptable shreshold for the loss value. In this way, we can find the optimal possible number of epochs to use for the model, likewise for the batch_size when matched with a specific epoch number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
