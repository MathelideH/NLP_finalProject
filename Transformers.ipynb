{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq6Fphct_x7F"
   },
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8urEVcpfO1A",
    "outputId": "7ee34167-1b3a-4544-87be-3e14a3c2188e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /opt/tljh/user/lib/python3.9/site-packages (4.28.1)\n",
      "Requirement already satisfied: requests in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (2023.3.23)\n",
      "Requirement already satisfied: filelock in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (3.11.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (0.13.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/tljh/user/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/tljh/user/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/tljh/user/lib/python3.9/site-packages (from requests->transformers) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "UlxtbvkcHJon"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import TFBertModel, BertTokenizer, DistilBertTokenizer, TFDistilBertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "tfxmsktEK2lO",
    "outputId": "60506e35-49a5-49e6-c0a3-0f4136d8936b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline   \n",
       "0             1  thirtysomething scientists unveil doomsday clo...  \\\n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json('sarcasm.json',lines=True)\n",
    "labels = data.is_sarcastic.values\n",
    "sentences = data.headline.values\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdJoKlIIdrux"
   },
   "source": [
    "# DistillBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "tT0ZKsSfJkrc"
   },
   "outputs": [],
   "source": [
    "train_sents,test_sents, train_labels, test_labels  = train_test_split(sentences,labels,test_size=0.2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dip8f0WBdrux",
    "outputId": "bd8a70ce-b11e-4b23-84e1-4dc6b7fd6500"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case = True)\n",
    "\n",
    "def encoder_dis(sentences):\n",
    "  ids = []\n",
    "  for sentence in sentences:\n",
    "    encoding = tokenizer.encode_plus(\n",
    "    sentence,\n",
    "    max_length=16,\n",
    "    truncation = True,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=False)\n",
    "    ids.append(encoding['input_ids'])\n",
    "  return ids\n",
    "\n",
    "train_ids_dis = encoder_dis(train_sents)\n",
    "test_ids_dis = encoder_dis(test_sents) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SICB4Wl9drux"
   },
   "outputs": [],
   "source": [
    "train_ids_dis = tf.convert_to_tensor(train_ids_dis)\n",
    "test_ids_dis = tf.convert_to_tensor(test_ids_dis)\n",
    "test_labels_dis = tf.convert_to_tensor(test_labels)\n",
    "train_labels_dis = tf.convert_to_tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xu_FffTgdruy",
    "outputId": "0e5f1b77-a3c2-4d5d-fb6a-71487c3988c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "distillbert_encoder = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "input_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \n",
    "embedding = distillbert_encoder([input_word_ids])\n",
    "dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dropout(0.2)(dense)   \n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n",
    "\n",
    "model_dis = tf.keras.Model(inputs=[input_word_ids], outputs=output)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "bfOn7lI5druy"
   },
   "outputs": [],
   "source": [
    "model_dis.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwEajDhFdruy",
    "outputId": "e9802f5b-4c25-4037-a455-ba0a09f1ac56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "609/609 [==============================] - 335s 549ms/step - loss: 0.1363 - accuracy: 0.9461 - val_loss: 0.3522 - val_accuracy: 0.8710\n",
      "Epoch 2/3\n",
      "609/609 [==============================] - 335s 550ms/step - loss: 0.0814 - accuracy: 0.9702 - val_loss: 0.4177 - val_accuracy: 0.8699\n",
      "Epoch 3/3\n",
      "609/609 [==============================] - 334s 549ms/step - loss: 0.0551 - accuracy: 0.9791 - val_loss: 0.3998 - val_accuracy: 0.8696\n"
     ]
    }
   ],
   "source": [
    "trained_dis = model_dis.fit(x = train_ids_dis, y = train_labels_dis, epochs = 3, verbose = 1, batch_size = 32, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In-distribution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 22s 122ms/step\n",
      "0.8815513626834381\n"
     ]
    }
   ],
   "source": [
    "predictions_indis = [1 * (x[0]>=0.5) for x in model_dis.predict(test_ids_dis)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis, predictions_indis)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6MEdQdpq6uW",
    "outputId": "ddae1c7f-2175-424d-b46f-7d5eeac7e4b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/opt/tljh/user/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#generate ids for reddit test data\n",
    "test_data1 = pd.read_csv('clean_reddit.csv')\n",
    "test_labels1 = test_data1.label.values\n",
    "test_sentences1 = test_data1.sent.values\n",
    "test_data1.head()\n",
    "\n",
    "\n",
    "test_ids1 = encoder_dis(test_sentences1)\n",
    "test_ids1 = tf.convert_to_tensor(test_ids1)\n",
    "test_labels_dis1 = tf.convert_to_tensor(test_labels1)\n",
    "# print(\"first 5 test_ids1: \", test_ids1[:5])\n",
    "# print(\"first 5 test_labels_dis1: \", test_labels_dis1[:5])\n",
    "\n",
    "\n",
    "#generate ids for tweet test data\n",
    "test_data2 = pd.read_csv('clean_tweet.csv')\n",
    "test_labels2 = test_data2.label.values\n",
    "test_sentences2 = test_data2.sent.values\n",
    "test_data2.head()\n",
    "\n",
    "\n",
    "test_ids2 = encoder_dis(test_sentences2)\n",
    "test_ids2 = tf.convert_to_tensor(test_ids2)\n",
    "test_labels_dis2 = tf.convert_to_tensor(test_labels2)\n",
    "# print(\"first 5 test_ids2: \", test_ids2[:5])\n",
    "# print(\"first 5 test_labels_dis2: \", test_labels_dis2[:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out-of-distribution Tests on Reddits and Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ll2x8aHJF9By",
    "outputId": "b7d70a38-3df5-415e-93fc-d426d215fb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 5s 121ms/step\n",
      "0.49135987978963186\n"
     ]
    }
   ],
   "source": [
    "predictions_reddit = [1 * (x[0]>=0.5) for x in model_dis.predict(test_ids1)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis1, predictions_reddit)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 6s 120ms/step\n",
      "0.5048409405255878\n"
     ]
    }
   ],
   "source": [
    "predictions_tweets = [1 * (x[0]>=0.5) for x in model_dis.predict(test_ids2)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis2, predictions_tweets)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Baseline Testing (Untrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_transform', 'activation_13', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "distillbert_encoder = TFDistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "input_word_ids = tf.keras.Input(shape=(16,), dtype=tf.int32, name=\"input_word_ids\")  \n",
    "embedding = distillbert_encoder([input_word_ids])\n",
    "dense = tf.keras.layers.Lambda(lambda seq: seq[:, 0, :])(embedding[0])\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
    "dense = tf.keras.layers.Dropout(0.2)(dense)   \n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)    \n",
    "\n",
    "model_base = tf.keras.Model(inputs=[input_word_ids], outputs=output)  \n",
    "model_base.compile(tf.keras.optimizers.Adam(1e-5), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 7s 121ms/step\n",
      "0.4883546205860255\n"
     ]
    }
   ],
   "source": [
    "predictions_reddit_base = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids1)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis1, predictions_reddit_base)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/46 [==============================] - 6s 119ms/step\n",
      "0.5214384508990318\n"
     ]
    }
   ],
   "source": [
    "predictions_tweet_base = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids2)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis2, predictions_tweet_base)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 22s 121ms/step\n",
      "0.5211390635918938\n"
     ]
    }
   ],
   "source": [
    "predictions_news = [1 * (x[0]>=0.5) for x in model_base.predict(test_ids_dis)]\n",
    "accuracy = metrics.accuracy_score(test_labels_dis, predictions_news)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
